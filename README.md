Dockerized LLM-Powered Document QA System

A containerized Retrieval-Augmented Generation (RAG) system that enables users to upload PDF documents and ask natural language questions. The system retrieves relevant document context using vector search (FAISS) and generates grounded answers using a locally hosted Large Language Model via Ollama.

ğŸš€ Overview

This project implements an end-to-end RAG pipeline including:

PDF ingestion

Text chunking

Embedding generation

Vector similarity search using FAISS

Context retrieval

LLM-based answer generation

Dockerized backend deployment

React-based frontend interface

The system ensures responses are grounded in uploaded documents rather than relying solely on the LLMâ€™s pre-trained knowledge.

ğŸ—ï¸ Architecture

User â†’ React Frontend â†’ FastAPI Backend â†’
PDF Ingestion â†’ Embeddings â†’ FAISS Vector Store â†’
Top-K Retrieval â†’ Local LLM (Ollama) â†’ Generated Answer

ğŸ› ï¸ Tech Stack
Backend

FastAPI (async API framework)

FAISS (vector similarity search)

Sentence Transformers (all-MiniLM-L6-v2 for embeddings)

Ollama (local LLM inference)

Docker & Docker Compose (containerization)

Frontend

React.js

Fetch API for backend communication

Models Used

Embedding Model:

sentence-transformers/all-MiniLM-L6-v2

Used to convert text chunks into dense vector embeddings.

LLM Model (via Ollama):

phi3 (or any locally available Ollama model)

Used for grounded response generation.

You can switch models by changing the model name in the backend configuration.

ğŸ“‚ Project Structure
Dockerized-LLM-Powered-Document-QA-System/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”œâ”€â”€ faiss_store.py
â”‚   â”‚   â”œâ”€â”€ retrieval.py
â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ rag-ui/
â”‚
â””â”€â”€ README.md
âš™ï¸ How It Works
1ï¸âƒ£ Document Upload

User uploads a PDF.

Text is extracted and split into manageable chunks.

2ï¸âƒ£ Embedding Generation

Each chunk is converted into vector embeddings using all-MiniLM-L6-v2.

3ï¸âƒ£ Vector Storage

Embeddings are stored in FAISS for fast similarity search.

4ï¸âƒ£ Question Answering

User asks a question.

Question is embedded.

Top-k similar chunks are retrieved.

Retrieved context is sent to the local LLM.

LLM generates a grounded answer.

ğŸ³ Running with Docker (Recommended)
Step 1: Start Backend
cd backend
docker compose up --build

Backend will run at:

http://localhost:8000

Swagger Docs:

http://localhost:8000/docs
Step 2: Start Frontend

In a separate terminal:

cd frontend/rag-ui
npm install
npm start

Frontend runs at:

http://localhost:3000
ğŸ–¥ï¸ Running Without Docker (Optional)
Backend
cd backend
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
uvicorn app.main:app --reload
Frontend
cd frontend/rag-ui
npm install
npm start
ğŸ“Œ Requirements

Python 3.10+

Node.js

Docker Desktop

Ollama installed locally

Install Ollama:
https://ollama.com

Pull model:

ollama pull phi4:latest

Start Ollama:

ollama run phi4:latest


âœ¨ Key Features

Full Retrieval-Augmented Generation pipeline

Vector-based semantic search

Local LLM inference (no external API dependency)

Dockerized backend deployment

Async FastAPI architecture

Interactive React frontend

ğŸ¯ Use Cases

Internal document QA systems

Enterprise knowledge assistants

Resume document analysis tools

Local AI assistants without cloud dependency

ğŸ“ˆ Future Improvements

Streaming responses

Multi-document support

Persistent vector database

Authentication & role-based access

Cloud deployment (AWS/GCP)

ğŸ‘©â€ğŸ’» Author

Shravani Vanalkar
Full-stack AI & Backend Developer